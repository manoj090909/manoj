# -*- coding: utf-8 -*-
"""CSCI567_id012.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qGU0bZatRrYNU-TX2rWPlxkXcDVpXIW_
"""

# Install Optuna for hyperparameter optimization
pip install optuna

# Import necessary libraries
import os
import numpy as np
import pandas as pd
import lightgbm as lgb
import keras
from keras.layers import *
from keras.optimizers import *
from keras.models import Model
from google.colab import drive
from sklearn.metrics import f1_score
from sklearn.model_selection import KFold
import optuna.integration.lightgbm as lgb_optuna
import optuna
from keras import backend as K
from sklearn.preprocessing import LabelEncoder
from scipy.stats.mstats import winsorize

# Mount Google Drive
drive.mount('/content/drive')

# Load datasets
train_x = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/train_values.csv")
train_y = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/train_labels.csv")
test_x = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/test_values.csv")
sub_csv = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/submission_format.csv")

train_x.columns

# Preprocess train_x by creating new columns and removing unnecessary ones
train_x["volume"]=train_x["area_percentage"]*train_x["height_percentage"]
train_x = train_x.drop(['area_percentage'],axis=1)
train_x = train_x.drop(['height_percentage'],axis=1)
train_x = train_x.drop(['has_secondary_use_institution'],axis=1)
train_x = train_x.drop(['has_secondary_use_school'],axis=1)
train_x = train_x.drop(['has_secondary_use_industry'],axis=1)
train_x = train_x.drop(['has_secondary_use_health_post'],axis=1)
train_x = train_x.drop(['has_secondary_use_gov_office'],axis=1)
train_x = train_x.drop(['has_secondary_use_use_police'],axis=1)
train_x = train_x.drop(['has_secondary_use_other'],axis=1)

# Winsorize 'age'column
data_age = train_x['age']
winsorized_data = winsorize(data_age,(0,0.05))
train_x['age']=winsorized_data

# Winsorize 'volume' column
data_volume = train_x['volume']
winsorized_data = winsorize(data_volume,(0,0.05))
train_x['volume']=winsorized_data

# One-hot encode geo level features
geo1 = np.array(pd.get_dummies(pd.concat([train_x["geo_level_1_id"], test_x["geo_level_1_id"]])))
geo2 = np.array(pd.get_dummies(pd.concat([train_x["geo_level_2_id"], test_x["geo_level_2_id"]])))
geo3 = np.array(pd.get_dummies(pd.concat([train_x["geo_level_3_id"], test_x["geo_level_3_id"]])))

geo3.shape

# Defining the neural network model for embedding
def NET():
    inp = Input((geo3.shape[1],))
    i1 = Dense(16, name="intermediate")(inp)
    x2 = Dense(geo2.shape[1], activation='sigmoid')(i1)
    x1 = Dense(geo1.shape[1], activation='sigmoid')(i1)

    model = Model(inp, [x2, x1])
    model.compile(loss="binary_crossentropy", optimizer="adam")
    return model

# Training the embedding model
model = NET()
model.fit(geo3, [geo2, geo1], batch_size=128, epochs=10, verbose=2)
model.save("geo_embed.h5")

# Load GEO-Embed Model
model = NET()
model.load_weights("geo_embed.h5")

get_int_layer_output = K.function([model.layers[0].input], [model.layers[1].output])

# A function to extract geo embeddings
def extract_geo_embeds(geo3_data, is_train=True):
    out = []
    for dat in geo3_data:
        layer_output = get_int_layer_output([np.expand_dims(dat, axis=0)])[0]
        out.append(layer_output)

    out = np.array(out)
    out = np.squeeze(out)

    return out

# A function to add geo embeddings to data
def add_geo_embeds_to_data(data, geo_embeds):
    data = pd.get_dummies(data.copy())
    data = data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)
    geo_feat_columns = [f'geo_feat{i}' for i in range(1, 17)]
    geo_embed_df = pd.DataFrame(geo_embeds, columns=geo_feat_columns)
    data = pd.concat([data, geo_embed_df], axis=1)

    return data

# Adding geo embeddings to train and test datasets
train_geo_embeds = extract_geo_embeds(geo3[:260601])
train_data = add_geo_embeds_to_data(train_x, train_geo_embeds)

test_geo_embeds = extract_geo_embeds(geo3[260601:], is_train=False)
test_data = add_geo_embeds_to_data(test_x, test_geo_embeds)

# Defining a function to threshold the predictions
def threshold_arr(array):
    new_arr = []
    for val in array:
        loc = np.array(val).argmax(axis=0)
        k = list(np.zeros((len(val))))
        k[loc] = 1
        new_arr.append(k)

    return np.array(new_arr)



# Initializing a random seed and save path for models
SEED = 42
save_path = "/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/models"
# Defining x and y
df = train_data.drop(["building_id"], axis=1)
x = np.array(df)
y = np.array(train_y["damage_grade"])-1

# Defining the objective function for Optuna optimization
def objective(trial, x_train, y_train, x_val, y_val):
    param = {
        "objective": "multiclass",
        "num_class": 3,
        "verbosity": -1,
        "boosting_type": 'dart',
        "num_leaves": trial.suggest_int("num_leaves", 10, 500),
        "max_depth": trial.suggest_int("max_depth", 1, 20),
        "learning_rate": trial.suggest_loguniform("learning_rate", 1e-8, 1.0),
        "feature_fraction": trial.suggest_uniform("feature_fraction", 0.1, 1.0),
        "seed": SEED
    }

    train_data = lgb.Dataset(x_train, label=y_train)
    val_data = lgb.Dataset(x_val, label=y_val)

    model = lgb.train(param, train_data, valid_sets=[val_data], verbose_eval=False, early_stopping_rounds=100)

    y_pred = model.predict(x_val, num_iteration=model.best_iteration)
    y_pred_labels = np.argmax(y_pred, axis=1)

    return f1_score(y_val, y_pred_labels, average='micro')

# Performing KFold cross-validation and Optuna optimization
kf = KFold(n_splits=5, shuffle=True, random_state=SEED)
for ix, (train_index, test_index) in enumerate(kf.split(x)):
    x_train, x_val, y_train, y_val = x[train_index], x[test_index], y[train_index], y[test_index]

    study = optuna.create_study(direction="maximize")
    study.optimize(lambda trial: objective(trial, x_train, y_train, x_val, y_val), n_trials=50)

    best_params = study.best_params
    best_params["objective"] = "multiclass"
    best_params["num_class"] = 3
    best_params["verbosity"] = -1
    best_params["seed"] = SEED

    lgb_train_data = lgb.Dataset(x_train, label=y_train)
    lgb_val_data   = lgb.Dataset(x_val, label=y_val)

    lgb_clf = lgb.train(best_params,
                    lgb_train_data,
                    20000,
                    valid_sets=[lgb_val_data],
                    early_stopping_rounds=3000,
                    verbose_eval=1000)

    y_pred = lgb_clf.predict(x_val)
    y_pred_labels = np.argmax(y_pred, axis=1)
    print("F1-MICRO SCORE: ", f1_score(y_val, y_pred_labels, average='micro'))

    lgb_clf.save_model(os.path.join(save_path, f'model{ix}.txt'))

# Loading the trained LightGBM models
models = []
for i in range(5):
    model = lgb.Booster(model_file=os.path.join(save_path, f'model{i}.txt'))
    y_pred = model.predict(x)
    y_pred_labels = np.argmax(y_pred, axis=1)
    score = f1_score(y, y_pred_labels, average='micro')
    print("F1-MICRO SCORE: ", score)
    models.append(model)

# Defining a function to make ensemble predictions
def ensemble(models, x):
    y_preds = []

    for model in models:
        y_pred = model.predict(x)
        y_preds.append(y_pred)

    init_y_pred = y_preds[0]
    for ypred in y_preds[1:]:
        init_y_pred += ypred

    y_pred = threshold_arr(init_y_pred)

    return y_pred

# Preprocessing test_data by creating new columns and removing unnecessary ones
df = test_data.drop(["building_id"], axis=1)
df["volume"]=df["area_percentage"]*df["height_percentage"]
df = df.drop(['area_percentage'],axis=1)
df = df.drop(['height_percentage'],axis=1)
df = df.drop(['has_secondary_use_institution'],axis=1)
df = df.drop(['has_secondary_use_school'],axis=1)
df = df.drop(['has_secondary_use_industry'],axis=1)
df = df.drop(['has_secondary_use_health_post'],axis=1)
df = df.drop(['has_secondary_use_gov_office'],axis=1)
df = df.drop(['has_secondary_use_use_police'],axis=1)
df = df.drop(['has_secondary_use_other'],axis=1)

# Make ensemble predictions on test dataset
x = np.array(df)

y_pred = ensemble(models, x)
y_pred = y_pred.argmax(axis=1) + 1

# Prepare submission file
sub_csv["damage_grade"] = y_pred
sub_csv.to_csv("submission.csv", index=False)

# Download the submission file
from google.colab import files
files.download("submission.csv")