# -*- coding: utf-8 -*-
"""richter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1az2Ny-tcTcWmZUkKujZdwYbZTE9m7oY2

# Dependecies
"""

import os 
import numpy as np
import pandas as pd

import lightgbm as lgb

import keras 
from keras.layers import *
from keras.optimizers import *
from keras.models import Model
from google.colab import drive

from sklearn.metrics import f1_score 
from sklearn.model_selection import KFold

drive.mount('/content/drive')

train_x = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/train_values.csv")
train_y = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/train_labels.csv")
test_x  = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/test_values.csv")
sub_csv = pd.read_csv("/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/submission_format.csv")

"""Under-Sampling"""

train = pd.merge(train_x, train_y, on="building_id")
train.head()

count_2,count_3,count_1 = train['damage_grade'].value_counts()
print(count_1,count_2,count_3)

(train.damage_grade.value_counts().plot.bar(title="Number of Buildings with Each Damage Grade"))

# Separate classes
damage_grade_1 = train[train['damage_grade'] == 1]
damage_grade_2 = train[train['damage_grade'] == 2]
damage_grade_3 = train[train['damage_grade'] == 3]

print(damage_grade_1.shape,damage_grade_2.shape,damage_grade_3.shape)

#Undersampling and Oversampling
damage_grade_1_over = damage_grade_1.sample(count_3,replace = True)
damage_grade_2_under = damage_grade_2.sample(count_3,replace = True)

new_train = pd.concat([damage_grade_1_over, damage_grade_2_under,damage_grade_3], axis=0)
(new_train.damage_grade.value_counts().plot.bar(title="Number of Buildings with Each Damage Grade"))

train_x = new_train.drop(['damage_grade'],axis=1)
train_y = new_train[['building_id','damage_grade']]

"""# Geographic Location ID Embedding w/ Autoencoder"""

geo1 = np.array(pd.get_dummies(pd.concat([train_x["geo_level_1_id"], test_x["geo_level_1_id"]])))
geo2 = np.array(pd.get_dummies(pd.concat([train_x["geo_level_2_id"], test_x["geo_level_2_id"]])))
geo3 = np.array(pd.get_dummies(pd.concat([train_x["geo_level_3_id"], test_x["geo_level_3_id"]])))

geo3.shape

def NET():
    inp = Input((geo3.shape[1],))
    i1 = Dense(16, name="intermediate")(inp)
    x2 = Dense(geo2.shape[1], activation='sigmoid')(i1)
    x1 = Dense(geo1.shape[1], activation='sigmoid')(i1)

    model = Model(inp, [x2,x1])
    model.compile(loss="binary_crossentropy", optimizer="adam")
    return model

model = NET()
model.fit(geo3, [geo2, geo1], batch_size=128, epochs=10, verbose=2)
model.save("geo_embed.h5")

# Load GEO-Embed Model
model = NET()
model.load_weights("geo_embed.h5")

# "Extract Intermediate Layer" Function
from keras import backend as K

get_int_layer_output = K.function([model.layers[0].input],
                                  [model.layers[1].output])

# Extract GEO-Embeds for all train data points.
# Then assign with train_data

out = []
for dat in geo3[:260601]:
    layer_output = get_int_layer_output([np.expand_dims(dat, axis=0)])[0]
    out.append(layer_output)

out = np.array(out)
out = np.squeeze(out)

train_data = pd.get_dummies(train_x.copy())
train_data = train_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)
train_data = train_data.assign(geo_feat1=out[:,0],
                               geo_feat2=out[:,1],
                               geo_feat3=out[:,2],  
                               geo_feat4=out[:,3],
                               geo_feat5=out[:,4],    
                               geo_feat6=out[:,5],
                               geo_feat7=out[:,6],
                               geo_feat8=out[:,7],
                               geo_feat9=out[:,8],
                               geo_feat10=out[:,9],
                               geo_feat11=out[:,10],
                               geo_feat12=out[:,11],
                               geo_feat13=out[:,12],
                               geo_feat14=out[:,13],
                               geo_feat15=out[:,14],           
                               geo_feat16=out[:,15])

train_data.head()

train_data.columns

# Extract GEO-Embeds for all test data points.
# Then assign with test_data

out = []
for dat in geo3[260601:]:
    layer_output = get_int_layer_output([np.expand_dims(dat, axis=0)])[0]
    out.append(layer_output)

out = np.array(out)
out = np.squeeze(out)

test_data = pd.get_dummies(test_x.copy())
test_data = test_data.drop(['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id'], axis=1)
test_data = test_data.assign(geo_feat1=out[:,0],
                             geo_feat2=out[:,1],
                             geo_feat3=out[:,2],  
                             geo_feat4=out[:,3],
                             geo_feat5=out[:,4],    
                             geo_feat6=out[:,5],
                             geo_feat7=out[:,6],
                             geo_feat8=out[:,7],
                             geo_feat9=out[:,8],
                             geo_feat10=out[:,9],
                             geo_feat11=out[:,10],
                             geo_feat12=out[:,11],
                             geo_feat13=out[:,12],
                             geo_feat14=out[:,13],
                             geo_feat15=out[:,14],           
                             geo_feat16=out[:,15])

test_data.head()

test_data.columns

def threshold_arr(array):
    # Get major confidence-scored predicted value.
    new_arr = []
    for ix, val in enumerate(array):
        loc = np.array(val).argmax(axis=0)
        k = list(np.zeros((len(val))))
        k[loc]=1
        new_arr.append(k)
        
    return np.array(new_arr)

"""# LightGBM Training"""



SEED = 1881
save_path = "/content/drive/MyDrive/CSCI 567- DataDriven Competition/Data/models"
y = np.array(train_y["damage_grade"])-1

df = train_data.drop(["building_id"], axis=1)
x = np.array(df)

kf = KFold(n_splits=5, shuffle=True, random_state=SEED)
for ix, (train_index, test_index) in enumerate(kf.split(x)):
    # lgb_params = {
    #     "objective" : "multiclass",
    #     "num_class":3,
    #     "metric" : "multi_error",
    #     "boosting": 'gbdt',
    #     "max_depth" : -1,
    #     "num_leaves" : 30,
    #     "learning_rate" : 0.1,
    #     "feature_fraction" : 0.5,
    #     "min_sum_hessian_in_leaf" : 0.1,
    #     "max_bin":8192,
    #     "verbosity" : 1,
    #     "num_threads":6,
    #     "seed": SEED
    # }
    lgb_params = {'learning_rate':.05,'boosting_type':'gbdt','objective':'multiclass','num_boost_round':1500,'num_leaves':300,'max_depth':10,'max_bin':8192,'num_class':3,
          'feature_fraction':0.5}

    x_train, x_val, y_train, y_val= x[train_index], x[test_index], y[train_index], y[test_index]

    train_data = lgb.Dataset(x_train, label=y_train)
    val_data   = lgb.Dataset(x_val, label=y_val)

    lgb_clf = lgb.train(lgb_params,
                        train_data,
                        20000,
                        valid_sets = [val_data],
                        early_stopping_rounds=3000,
                        verbose_eval = 1000)

    y_pred = lgb_clf.predict(x_val)
    
    y_pred_labels = np.argmax(y_pred, axis=1)
    print("F1-MICRO SCORE: ", f1_score(y_val, y_pred_labels, average='micro'))

    lgb_clf.save_model(os.path.join(save_path, f'model{ix}.txt'))



"""# Create Submission File"""

# Load all LightGB Models and concatenate.
models = []
for i in range(5):
    model = lgb.Booster(model_file=os.path.join(save_path, f'model{i}.txt'))

    y_pred = model.predict(x)
    
    y_pred_labels = np.argmax(y_pred, axis=1)
    score  = f1_score(y, y_pred_labels, average='micro')
    print("F1-MICRO SCORE: ", score)
    models.append(model)

def ensemble(models, x):
    # Ensemble K-Fold CV models with adding all confidence score by class.
    y_preds = []
    
    for model in models:
        y_pred = model.predict(x)
        y_preds.append(y_pred)
        
    init_y_pred = y_preds[0]
    for ypred in y_preds[1:]:
        init_y_pred += ypred
        
    y_pred = threshold_arr(init_y_pred)
    
    return y_pred

df = test_data.drop(["building_id"], axis=1)
x = np.array(df)

y_pred = ensemble(models, x)
y_pred = y_pred.argmax(axis=1)+1

sub_csv["damage_grade"] = y_pred
sub_csv.to_csv("submission.csv", index=False)

from google.colab import files
files.download("submission.csv")